# -*- coding: utf-8 -*-
"""Main basepaper1

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ww2B41-KLmrUmKUmmdEQVqCYvLrxlsW8
"""

import subprocess
import sys

# Install catboost
subprocess.check_call([sys.executable, "-m", "pip", "install", "catboost"])

# Basic Libraries
import pandas as pd
import numpy as np

# Preprocessing Libraries
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# Handle Imbalance
from imblearn.over_sampling import SMOTE

# ML Models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import GaussianNB
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier

# Ensemble Techniques
from sklearn.ensemble import BaggingClassifier, ExtraTreesClassifier, VotingClassifier

# Suppress warnings
import warnings
warnings.filterwarnings("ignore")

#Load and Inspect the Dataset

# Load the dataset
df = pd.read_csv("water_potability.csv")

# Inspect the dataset
print("Dataset Information:")
print(df.info())

print("\nFirst 5 rows of the dataset:")
print(df.head())

# Check missing values
print("\nMissing values in each column:")
print(df.isnull().sum())

# HANDLING MISSING VALUES

# Fill missing values with column means
df = df.fillna(df.mean())

# Verify missing values are handled
print("Missing values after filling:")
print(df.isnull().sum())

#Check for Outliers: Use boxplots or z-scores to detect and optionally remove outliers.

import matplotlib.pyplot as plt
import seaborn as sns

# Boxplot for pH values
sns.boxplot(df['ph'])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Boxplot for pH values
sns.boxplot(df['Hardness'])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Boxplot for pH values
sns.boxplot(df['Solids'])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Boxplot for pH values
sns.boxplot(df['Chloramines'])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Boxplot for pH values
sns.boxplot(df['Sulfate'])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Boxplot for pH values
sns.boxplot(df['Conductivity'])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Boxplot for pH values
sns.boxplot(df['Organic_carbon'])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Boxplot for pH values
sns.boxplot(df['Trihalomethanes'])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Boxplot for pH values
sns.boxplot(df['Turbidity'])
plt.show()

import matplotlib.pyplot as plt
import seaborn as sns

# Boxplot for pH values
sns.boxplot(df['Potability'])
plt.show()

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Simulate dataset based on the provided information
data = {
    "ph": [None, 3.716080, 8.099124, 8.316766, 9.092223],
    "Hardness": [204.890455, 129.422921, 224.236259, 214.373394, 181.101509],
    "Solids": [20791.318981, 18630.057858, 19909.541732, 22018.417441, 17978.986339],
    "Chloramines": [7.300212, 6.635246, 9.275884, 8.059332, 6.546600],
    "Sulfate": [368.516441, None, None, 356.886136, 310.135738],
    "Conductivity": [564.308654, 592.885359, 418.606213, 363.266516, 398.410813],
    "Organic_carbon": [10.379783, 15.180013, 16.868637, 18.436524, 11.558279],
    "Trihalomethanes": [86.990970, 56.329076, 66.420093, 100.341674, 31.997993],
    "Turbidity": [2.963135, 4.500656, 3.055934, 4.628771, 4.075075],
    "Potability": [0, 0, 0, 0, 0],
}

# Create a DataFrame
df = pd.DataFrame(data)

# Add missing rows to simulate the dataset size
import numpy as np
np.random.seed(0)
rows_to_add = 3276 - len(df)
new_rows = {
    "ph": np.append(np.random.uniform(6.5, 8.5, rows_to_add - 5), [15, 16, 17, 18, 20]),  # Add outliers
    "Hardness": np.random.uniform(100, 300, rows_to_add),
    "Solids": np.random.uniform(15000, 30000, rows_to_add),
    "Chloramines": np.random.uniform(4, 10, rows_to_add),
    "Sulfate": np.random.uniform(250, 400, rows_to_add),
    "Conductivity": np.random.uniform(350, 600, rows_to_add),
    "Organic_carbon": np.random.uniform(10, 20, rows_to_add),
    "Trihalomethanes": np.random.uniform(50, 100, rows_to_add),
    "Turbidity": np.random.uniform(2, 5, rows_to_add),
    "Potability": np.random.choice([0, 1], rows_to_add),
}
new_df = pd.DataFrame(new_rows)
df = pd.concat([df, new_df], ignore_index=True)

# Boxplot grid
features = ["ph", "Hardness", "Solids", "Chloramines", "Sulfate", "Conductivity", "Organic_carbon", "Trihalomethanes", "Turbidity"]
fig, axes = plt.subplots(3, 3, figsize=(15, 12))
axes = axes.flatten()

for i, feature in enumerate(features):
    sns.boxplot(
        data=df,
        x="Potability",
        y=feature,
        ax=axes[i],
        palette="Set2",
        showfliers=True,  # Ensure outliers are shown
        flierprops={"marker": "o", "color": "black", "alpha": 0.7}  # Customize outlier appearance
    )
    axes[i].set_title(feature)

# Adjust layout
plt.tight_layout()
plt.suptitle("Feature distribution by potability class and approved limit", y=1.02)
plt.show()

#Normalize Data

#normalize data
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df.drop('Potability', axis=1)),
                         columns=df.columns[:-1])
df_scaled['Potability'] = df['Potability']

#SPLIT FEATURES& TARGET

#SPLIT FEATURES& TARGET
# Define features and target
X = df.drop('Potability', axis=1)
y = df['Potability']

# Scale the features
scaler = MinMaxScaler()
X_scaled = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)

# Display scaled data
print("\nScaled Features:")
print(X_scaled.head())

#Exploratory Data Analysis (EDA)
#Analyze feature distributions
df.hist(figsize=(12, 10))
plt.show()

# CORRELATION ANALYSIS
sns.heatmap(df.corr(), annot=True, cmap='coolwarm')
plt.show()

#CHECK TARGET BALANCE

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Load the dataset
df = pd.read_csv('/content/water_potability.csv')

# Plot with different colors for 0 and 1 (as strings)
ax = sns.countplot(x='Potability', data=df, palette={'0': 'skyblue', '1': 'lightcoral'})

# Calculate total number of samples
total = len(df)

# Add percentage labels to the bars
for p in ax.patches:
    percentage = f"{100 * p.get_height() / total:.2f}%"
    ax.annotate(percentage, (p.get_x() + p.get_width() / 2, p.get_height()),
                ha='center', va='bottom', fontsize=12)

# Show plot
plt.title("Potability Distribution")
plt.show()

# SPLIT DATA INTO TRAIN & TEST SETS

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42, stratify=y)

# Check class distribution
print("\nClass Distribution in Training Data:")
print(y_train.value_counts())

# Apply SMOTE for Class Imbalance

# Apply SMOTE to balance the training set
smote = SMOTE(random_state=42)
X_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)

# Verify class distribution after SMOTE
print("\nClass Distribution After SMOTE:")
print(y_train_smote.value_counts())

#MODEL SELECTION AND TRAINING

#MODEL SELECTION AND TRAINING
# Dictionary to store results
results = {}

# 1. Logistic Regression
lr = LogisticRegression()
lr.fit(X_train_smote, y_train_smote)
y_pred_lr = lr.predict(X_test)
results['Logistic Regression'] = accuracy_score(y_test, y_pred_lr)

# 2. Decision Tree
dt = DecisionTreeClassifier(random_state=42)
dt.fit(X_train_smote, y_train_smote)
y_pred_dt = dt.predict(X_test)
results['Decision Tree'] = accuracy_score(y_test, y_pred_dt)

# 3. Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train_smote, y_train_smote)
y_pred_rf = rf.predict(X_test)
results['Random Forest'] = accuracy_score(y_test, y_pred_rf)

# 4. Gradient Boosting
gb = GradientBoostingClassifier(random_state=42)
gb.fit(X_train_smote, y_train_smote)
y_pred_gb = gb.predict(X_test)
results['Gradient Boosting'] = accuracy_score(y_test, y_pred_gb)

# 5. AdaBoost
ab = AdaBoostClassifier(random_state=42)
ab.fit(X_train_smote, y_train_smote)
y_pred_ab = ab.predict(X_test)
results['AdaBoost'] = accuracy_score(y_test, y_pred_ab)

# 6. Support Vector Machine
svm = SVC(kernel='rbf', random_state=42)
svm.fit(X_train_smote, y_train_smote)
y_pred_svm = svm.predict(X_test)
results['SVM'] = accuracy_score(y_test, y_pred_svm)

# 7. K-Nearest Neighbors
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train_smote, y_train_smote)
y_pred_knn = knn.predict(X_test)
results['KNN'] = accuracy_score(y_test, y_pred_knn)

# 8. Gaussian Naive Bayes
gnb = GaussianNB()
gnb.fit(X_train_smote, y_train_smote)
y_pred_gnb = gnb.predict(X_test)
results['Naive Bayes'] = accuracy_score(y_test, y_pred_gnb)

# 9. Bagging Classifier
bagging = BaggingClassifier(random_state=42)
bagging.fit(X_train_smote, y_train_smote)
y_pred_bag = bagging.predict(X_test)
results['Bagging Classifier'] = accuracy_score(y_test, y_pred_bag)

# 10. Extra Trees Classifier
et = ExtraTreesClassifier(random_state=42)
et.fit(X_train_smote, y_train_smote)
y_pred_et = et.predict(X_test)
results['Extra Trees'] = accuracy_score(y_test, y_pred_et)

# 11. Voting Classifier
voting = VotingClassifier(
    estimators=[('rf', rf), ('gb', gb), ('svm', svm)], voting='hard')
voting.fit(X_train_smote, y_train_smote)
y_pred_voting = voting.predict(X_test)
results['Voting Classifier'] = accuracy_score(y_test, y_pred_voting)

# 12. XGBoost
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb.fit(X_train_smote, y_train_smote)
y_pred_xgb = xgb.predict(X_test)
results['XGBoost'] = accuracy_score(y_test, y_pred_xgb)

# 13. CatBoost
cat = CatBoostClassifier(verbose=0, random_state=42)
cat.fit(X_train_smote, y_train_smote)
y_pred_cat = cat.predict(X_test)
results['CatBoost'] = accuracy_score(y_test, y_pred_cat)

# 14. LGBMClassifier
lgbm = LGBMClassifier(random_state=42)
lgbm.fit(X_train_smote, y_train_smote)
y_pred_lgbm = lgbm.predict(X_test)
results['LGBM'] = accuracy_score(y_test, y_pred_lgbm)

# Display Results
print("\nModel Performance Comparison:")
for model, acc in results.items():
    print(f"{model}: {acc:.4f}")

import matplotlib.pyplot as plt

# Display model performance results
print("\nModel Performance Comparison:")
for model, acc in results.items():
    print(f"{model}: Accuracy = {acc:.4f}")

# Sort models by accuracy for better comparison
sorted_results = dict(sorted(results.items(), key=lambda x: x[1], reverse=True))

# Visualize Model Performance
plt.figure(figsize=(10, 6))
plt.barh(list(sorted_results.keys()), list(sorted_results.values()), color='skyblue')
plt.xlabel("Accuracy")
plt.title("Model Performance Comparison")
plt.gca().invert_yaxis()  # Invert axis to show the best model at the top
plt.show()

#XGBOOST

import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from xgboost import XGBClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load dataset
df = pd.read_csv('/content/water_potability.csv')

# Handling missing values - fill with mean for features and mode for target
df.fillna(df.mean(), inplace=True)

# Separate features and target
X = df.drop('Potability', axis=1)
y = df['Potability']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (scaling both train and test sets)
scaler = StandardScaler()

# Fit the scaler on the training set and transform both the training and test sets
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)

# Check for any NaN values in resampled data (using numpy method)
print("Any missing values in resampled data?", np.isnan(X_train_res).sum() == 0)

# Initialize XGBoost classifier
xgb = XGBClassifier(random_state=42)

# Train the XGBoost model
xgb.fit(X_train_res, y_train_res)

# Predict on the test set
y_pred_xgb = xgb.predict(X_test_scaled)

# Evaluate the model
print(f"XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb)}")
print("XGBoost Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_xgb))
print("XGBoost Classification Report:")
print(classification_report(y_test, y_pred_xgb))

# 1. Confusion Matrix
def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Non-potable", "Potable"], yticklabels=["Non-potable", "Potable"])
    plt.title(title)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# Plot confusion matrix for XGBoost
plot_confusion_matrix(y_test, y_pred_xgb, title='XGBoost Confusion Matrix')

# 2. ROC Curve
def plot_roc_curve(y_true, y_pred_prob, title='ROC Curve'):
    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='b', label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
    plt.title(title)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.show()

# Get predicted probabilities for ROC
y_pred_prob_xgb = xgb.predict_proba(X_test_scaled)[:, 1]

# Plot ROC curve for XGBoost
plot_roc_curve(y_test, y_pred_prob_xgb, title='XGBoost ROC Curve')

# 3. Feature Importance
def plot_feature_importance(model, X, title='Feature Importance'):
    importance = model.feature_importances_
    feature_names = X.columns
    feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importance})
    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_importance)
    plt.title(title)
    plt.show()

# Plot feature importance for XGBoost
plot_feature_importance(xgb, X, title='XGBoost Feature Importance')

#to improve the performance of model using hyperparameters tuning

!pip install -U scikit-learn==1.2.2

# Step 1: Install necessary libraries
!pip install imbalanced-learn

# Step 2: Import libraries
import pandas as pd
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from xgboost import XGBClassifier
from imblearn.over_sampling import SMOTE

# Step 3: Load the dataset
from google.colab import files
uploaded = files.upload()  #

# Step 4: Read the dataset
data = pd.read_csv('water_potability.csv')

# Step 5: Check for missing values and handle them
print("Missing values in dataset:\n", data.isnull().sum())
data = data.dropna()  # Drop rows with missing values for simplicity

# Step 6: Define features (X) and target (y)
X = data.drop('Potability', axis=1)  # Features
y = data['Potability']  # Target variable

# Step 7: Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 8: Handle class imbalance using SMOTE
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Step 9: Scale the data
scaler = StandardScaler()
X_train_res = scaler.fit_transform(X_train_res)
X_test_scaled = scaler.transform(X_test)

# Step 10: Define XGBoost model
xgb = XGBClassifier(
    random_state=42,
    use_label_encoder=False,
    eval_metric='logloss'
)

# Step 11: Define hyperparameters for tuning
param_grid_xgb = {
    'learning_rate': [0.01, 0.1, 0.2],
    'max_depth': [3, 6, 9],
    'n_estimators': [100, 200],
    'subsample': [0.8, 1.0],
    'colsample_bytree': [0.8, 1.0]
}

# Step 12: Perform Grid Search with Cross-Validation
grid_search_xgb = GridSearchCV(
    estimator=xgb,
    param_grid=param_grid_xgb,
    cv=3,
    scoring='accuracy',
    n_jobs=-1,
    verbose=1
)

# Step 13: Fit the Grid Search on training data
grid_search_xgb.fit(X_train_res, y_train_res)

# Step 14: Output the best parameters and score
print("Best parameters for XGBoost: ", grid_search_xgb.best_params_)
print("Best score for XGBoost: ", grid_search_xgb.best_score_)

# Step 15: Train the best model
best_xgb = grid_search_xgb.best_estimator_
y_pred_xgb_tuned = best_xgb.predict(X_test_scaled)

# Step 16: Evaluate the tuned model
from sklearn.metrics import classification_report, confusion_matrix
print(f"Tuned XGBoost Accuracy: {accuracy_score(y_test, y_pred_xgb_tuned)}")
print("\nClassification Report:\n", classification_report(y_test, y_pred_xgb_tuned))
print("\nConfusion Matrix:\n", confusion_matrix(y_test, y_pred_xgb_tuned))

#LGBM

import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from lightgbm import LGBMClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load dataset
df = pd.read_csv('/content/water_potability.csv')
# Handling missing values - fill with mean for features and mode for target
df.fillna(df.mean(), inplace=True)

# Separate features and target
X = df.drop('Potability', axis=1)
y = df['Potability']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (scaling both train and test sets)
scaler = StandardScaler()

# Fit the scaler on the training set and transform both the training and test sets
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)

# Check for any NaN values in resampled data (using numpy method)
print("Any missing values in resampled data?", np.isnan(X_train_res).sum() == 0)

# Initialize LGBM classifier
lgbm = LGBMClassifier(random_state=42)

# Train the LGBM model
lgbm.fit(X_train_res, y_train_res)

# Predict on the test set
y_pred_lgbm = lgbm.predict(X_test_scaled)

# Evaluate the model
print(f"LGBM Accuracy: {accuracy_score(y_test, y_pred_lgbm)}")
print("LGBM Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_lgbm))
print("LGBM Classification Report:")
print(classification_report(y_test, y_pred_lgbm))

# 1. Confusion Matrix
def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Non-potable", "Potable"], yticklabels=["Non-potable", "Potable"])
    plt.title(title)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# Plot confusion matrix for LGBM
plot_confusion_matrix(y_test, y_pred_lgbm, title='LGBM Confusion Matrix')

# 2. ROC Curve
def plot_roc_curve(y_true, y_pred_prob, title='ROC Curve'):
    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='b', label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
    plt.title(title)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.show()

# Get predicted probabilities for ROC
y_pred_prob_lgbm = lgbm.predict_proba(X_test_scaled)[:, 1]

# Plot ROC curve for LGBM
plot_roc_curve(y_test, y_pred_prob_lgbm, title='LGBM ROC Curve')

# 3. Feature Importance
def plot_feature_importance(model, X, title='Feature Importance'):
    importance = model.feature_importances_
    feature_names = X.columns
    feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importance})
    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_importance)
    plt.title(title)
    plt.show()

# Plot feature importance for LGBM
plot_feature_importance(lgbm, X, title='LGBM Feature Importance')

#LGBM Hyperparameter Tuning

from sklearn.model_selection import GridSearchCV
from lightgbm import LGBMClassifier

# Define LGBM model
lgbm = LGBMClassifier(random_state=42)

# Define hyperparameters for tuning
param_grid_lgbm = {
    'learning_rate': [0.01, 0.05, 0.1],
    'num_leaves': [31, 50, 100],
    'n_estimators': [100, 200],
    'max_depth': [6, 8, 10]
}

# Perform Grid Search
grid_search_lgbm = GridSearchCV(estimator=lgbm, param_grid=param_grid_lgbm, cv=3, scoring='accuracy', n_jobs=-1, verbose=1)
grid_search_lgbm.fit(X_train_res, y_train_res)

# Best parameters and score
print("Best parameters for LGBM: ", grid_search_lgbm.best_params_)
print("Best score for LGBM: ", grid_search_lgbm.best_score_)

# Train the best model
best_lgbm = grid_search_lgbm.best_estimator_
y_pred_lgbm_tuned = best_lgbm.predict(X_test_scaled)

# Evaluate the tuned model
print(f"Tuned LGBM Accuracy: {accuracy_score(y_test, y_pred_lgbm_tuned)}")

#CATBOOST

import pandas as pd
from imblearn.over_sampling import SMOTE
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Load dataset
df = pd.read_csv('/content/water_potability.csv')

# Handling missing values - fill with mean for features and mode for target
df.fillna(df.mean(), inplace=True)

# Separate features and target
X = df.drop('Potability', axis=1)
y = df['Potability']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling (scaling both train and test sets)
scaler = StandardScaler()

# Fit the scaler on the training set and transform both the training and test sets
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply SMOTE to handle class imbalance
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)

# Check for any NaN values in resampled data (using numpy method)
print("Any missing values in resampled data?", np.isnan(X_train_res).sum() == 0)

# Initialize CatBoost classifier
catboost = CatBoostClassifier(random_state=42, iterations=1000, learning_rate=0.1, depth=6, verbose=0)

# Train the CatBoost model
catboost.fit(X_train_res, y_train_res)

# Predict on the test set
y_pred_catboost = catboost.predict(X_test_scaled)

# Evaluate the model
print(f"CatBoost Accuracy: {accuracy_score(y_test, y_pred_catboost)}")
print("CatBoost Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_catboost))
print("CatBoost Classification Report:")
print(classification_report(y_test, y_pred_catboost))

# 1. Confusion Matrix
def plot_confusion_matrix(y_true, y_pred, title='Confusion Matrix'):
    cm = confusion_matrix(y_true, y_pred)
    plt.figure(figsize=(6, 5))
    sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=["Non-potable", "Potable"], yticklabels=["Non-potable", "Potable"])
    plt.title(title)
    plt.xlabel('Predicted')
    plt.ylabel('True')
    plt.show()

# Plot confusion matrix for CatBoost
plot_confusion_matrix(y_test, y_pred_catboost, title='CatBoost Confusion Matrix')

# 2. ROC Curve
def plot_roc_curve(y_true, y_pred_prob, title='ROC Curve'):
    fpr, tpr, _ = roc_curve(y_true, y_pred_prob)
    roc_auc = auc(fpr, tpr)
    plt.figure(figsize=(8, 6))
    plt.plot(fpr, tpr, color='b', label=f'ROC curve (AUC = {roc_auc:.2f})')
    plt.plot([0, 1], [0, 1], color='gray', linestyle='--')
    plt.title(title)
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.legend(loc='lower right')
    plt.show()

# Get predicted probabilities for ROC
y_pred_prob_catboost = catboost.predict_proba(X_test_scaled)[:, 1]

# Plot ROC curve for CatBoost
plot_roc_curve(y_test, y_pred_prob_catboost, title='CatBoost ROC Curve')

# 3. Feature Importance
def plot_feature_importance(model, X, title='Feature Importance'):
    importance = model.get_feature_importance()
    feature_names = X.columns
    feature_importance = pd.DataFrame({'Feature': feature_names, 'Importance': importance})
    feature_importance = feature_importance.sort_values(by='Importance', ascending=False)

    plt.figure(figsize=(10, 6))
    sns.barplot(x='Importance', y='Feature', data=feature_importance)
    plt.title(title)
    plt.show()

# Plot feature importance for CatBoost
plot_feature_importance(catboost, X, title='CatBoost Feature Importance')

#CatBoost Hyperparameter Tuning

from sklearn.model_selection import GridSearchCV, train_test_split
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from catboost import CatBoostClassifier
from sklearn.metrics import accuracy_score
import pandas as pd


# Load dataset
df = pd.read_csv('/content/water_potability.csv')

# Handle missing values (fill with mean for features)
df.fillna(df.mean(), inplace=True)

# Separate features and target
X = df.drop('Potability', axis=1)
y = df['Potability']

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Feature scaling
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Apply SMOTE for class imbalance
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train_scaled, y_train)

# Define CatBoost model
catboost = CatBoostClassifier(random_state=42, verbose=0)

# Define hyperparameters for tuning
param_grid_catboost = {
    'learning_rate': [0.01, 0.1, 0.2],
    'depth': [4, 6, 8],
    'iterations': [500, 1000],
    'l2_leaf_reg': [1, 3, 5]
}

# Perform Grid Search with Cross-Validation
grid_search_catboost = GridSearchCV(
    estimator=catboost,
    param_grid=param_grid_catboost,
    cv=3,
    scoring='accuracy',
    n_jobs=-1,
    verbose=0
)

# Fit the model with resampled training data
grid_search_catboost.fit(X_train_res, y_train_res)

# Best parameters and score
print("Best parameters for CatBoost: ", grid_search_catboost.best_params_)
print("Best score for CatBoost (CV Accuracy): ", grid_search_catboost.best_score_)

# Train the best model on resampled training data
best_catboost = grid_search_catboost.best_estimator_

# Predict on the scaled test set
y_pred_catboost_tuned = best_catboost.predict(X_test_scaled)

# Evaluate the tuned model
print(f"Tuned CatBoost Accuracy on Test Set: {accuracy_score(y_test, y_pred_catboost_tuned):.4f}")

#DETAILED EVALUATION FOR CATBOOST,XGBOOST AND LGBM MODELS
#Using metrics like confusion matrix, classification report, and ROC-AUC.

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# SMOTE applied only to the training data
smote = SMOTE(random_state=42)
X_train_res, y_train_res = smote.fit_resample(X_train, y_train)

# Train models
xgb_model = XGBClassifier(random_state=42)
xgb_model.fit(X_train_res, y_train_res)

catboost_model = CatBoostClassifier(verbose=0, random_state=42)
catboost_model.fit(X_train_res, y_train_res)

lgbm_model = LGBMClassifier(random_state=42)
lgbm_model.fit(X_train_res, y_train_res)

# Predictions
y_pred_xgb = xgb_model.predict(X_test)
y_pred_cat = catboost_model.predict(X_test)
y_pred_lgbm = lgbm_model.predict(X_test)

# Check lengths
print(len(y_test))        # Should print the same number for all predictions
print(len(y_pred_xgb))
print(len(y_pred_cat))
print(len(y_pred_lgbm))

# Plot confusion matrices
plot_confusion_matrix(y_test, y_pred_xgb, "XGBoost")
plot_confusion_matrix(y_test, y_pred_cat, "CatBoost")
plot_confusion_matrix(y_test, y_pred_lgbm, "LGBM")

#COMAPRE ROC CURVES
#helps compare the performance of models in terms of True Positive Rate (TPR) vs False Positive Rate (FPR).

!pip install catboost

from sklearn.model_selection import train_test_split
from sklearn.metrics import roc_curve, roc_auc_score
import matplotlib.pyplot as plt
from xgboost import XGBClassifier
from catboost import CatBoostClassifier
from lightgbm import LGBMClassifier

# Ensure the data is loaded
df = pd.read_csv('/content/water_potability.csv')

# Separate features (X) and target (y)
X = df.drop('Potability', axis=1)
y = df['Potability']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the models
xgb = XGBClassifier(random_state=42)
cat = CatBoostClassifier(random_state=42, verbose=0)
lgbm = LGBMClassifier(random_state=42)

# Fit the models to training data
xgb.fit(X_train, y_train)
cat.fit(X_train, y_train)
lgbm.fit(X_train, y_train)

# Calculate ROC curves
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, xgb.predict_proba(X_test)[:, 1])
fpr_cat, tpr_cat, _ = roc_curve(y_test, cat.predict_proba(X_test)[:, 1])
fpr_lgbm, tpr_lgbm, _ = roc_curve(y_test, lgbm.predict_proba(X_test)[:, 1])

# Calculate AUC scores
roc_auc_xgb = roc_auc_score(y_test, xgb.predict_proba(X_test)[:, 1])
roc_auc_cat = roc_auc_score(y_test, cat.predict_proba(X_test)[:, 1])
roc_auc_lgbm = roc_auc_score(y_test, lgbm.predict_proba(X_test)[:, 1])

# Plot the ROC curves
plt.figure(figsize=(8, 6))
plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {roc_auc_xgb:.4f})', color='blue')
plt.plot(fpr_cat, tpr_cat, label=f'CatBoost (AUC = {roc_auc_cat:.4f})', color='green')
plt.plot(fpr_lgbm, tpr_lgbm, label=f'LGBM (AUC = {roc_auc_lgbm:.4f})', color='red')
plt.plot([0, 1], [0, 1], linestyle="--", color="gray")  # Diagonal line for random guessing

# Add plot labels and title
plt.xlabel("False Positive Rate (FPR)")
plt.ylabel("True Positive Rate (TPR)")
plt.title("ROC Curves for Top 3 Models")
plt.legend(loc='lower right')
plt.grid(True)
plt.show()

#top-performing MODELS based on accuracy and ROC-AUC scores.

from sklearn.metrics import accuracy_score

# Initialize the models
models = {
    'XGBoost': xgb,
    'CatBoost': cat,
    'LGBM': lgbm
}

# Dictionary to store model accuracy scores
results = {}

# Evaluate each model and store its accuracy
for model_name, model in models.items():
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results[model_name] = acc

# Sort the results by accuracy
sorted_results = dict(sorted(results.items(), key=lambda item: item[1], reverse=True))

# Print the performance summary sorted by accuracy
print("\nFinal Model Performance Summary:")
print("Sorted by Accuracy:")
for model, acc in sorted_results.items():
    print(f"{model}: {acc:.4f}")

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report

# After training, save the model
cat.save_model('catboost_model.cbm')

# Predict on test data
y_pred_catboost = cat.predict(X_test)

# Evaluate the model
print(f"Accuracy: {accuracy_score(y_test, y_pred_catboost)}")
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred_catboost))
print("Classification Report:")
print(classification_report(y_test, y_pred_catboost))

#ADDITIONAL MODELS AND TECHNIQUES TO INCREASE ACCURACY OF WATER

#1]ensemble techniques

# a.STACKING

from sklearn.ensemble import StackingClassifier
from sklearn.linear_model import LogisticRegression

estimators = [
    ('xgb', XGBClassifier()),
    ('cat', CatBoostClassifier(verbose=0)),
    ('lgbm', LGBMClassifier())
]
stack_model = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())
stack_model.fit(X_train, y_train)

#b)Voting Classifier

from sklearn.ensemble import VotingClassifier

voting_model = VotingClassifier(estimators=estimators, voting='soft')
voting_model.fit(X_train, y_train)

#2]Neural Networks
#a.Multi-Layer Perceptron (MLP):

from sklearn.impute import SimpleImputer
from sklearn.neural_network import MLPClassifier

# Impute missing values
imputer = SimpleImputer(strategy='mean')
X_train = imputer.fit_transform(X_train)

# Train the MLPClassifier
mlp = MLPClassifier(hidden_layer_sizes=(128, 64, 32), max_iter=500, random_state=42)
mlp.fit(X_train, y_train)

#b.Long Short-Term Memory (LSTM):

#3]Advanced Boosting Algorithms
#a.AdaBoost (Adaptive Boosting)

from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(n_estimators=200, learning_rate=0.5)
ada.fit(X_train, y_train)

#b.Gradient Boosting Classifier

from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier(n_estimators=300, learning_rate=0.1, max_depth=3)
gbc.fit(X_train, y_train)

#AutoML Frameworks

#a.Auto-Sklearn

!apt-get update
!apt-get install -y swig cmake python3-dev build-essential

!pip install auto-sklearn

#PyCaret AutoML library

# Install PyCaret
!pip install pycaret pandas numpy scikit-learn

# Import required libraries
import pandas as pd
from pycaret.classification import *

# Load the water potability dataset
from google.colab import files

print("Please upload the 'water_potability.csv' file:")
uploaded = files.upload()

# Read the dataset
df = pd.read_csv('water_potability.csv')

# Display the first few rows to understand the data
print("\nFirst 5 rows of the dataset:")
print(df.head())

# Display basic information about the dataset
print("\nDataset Information:")
print(df.info())

# Handle missing values by replacing them with the column mean
df.fillna(df.mean(), inplace=True)

# Initialize PyCaret setup
clf = setup(data=df, target='Potability', session_id=42, train_size=0.8)

# Compare multiple models and select the best one automatically
best_model = compare_models()

# Display the best model
print("\nBest Model Selected:")
print(best_model)

# Evaluate the best model
evaluate_model(best_model)

# Finalize the model (train on the entire dataset)
final_model = finalize_model(best_model)

# Predict on the test set
predictions = predict_model(final_model)

# Export the best model pipeline
save_model(final_model, 'best_water_potability_model')

print("\nThe best model pipeline has been saved as 'best_water_potability_model'.")

#TRAIN AND EVALUATE EXTRA TREES CLASSIFIER

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.ensemble import ExtraTreesClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay

import pandas as pd

# Load the dataset from the local Colab workspace
df = pd.read_csv('water_potability.csv')

# Display the first few rows
print(df.head())

# Fill missing values with the mean of each column
df.fillna(df.mean(), inplace=True)

# Features and target
X = df.drop('Potability', axis=1)
y = df['Potability']

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize and train the Extra Trees Classifier
et_model = ExtraTreesClassifier(n_estimators=200, random_state=42)
et_model.fit(X_train, y_train)

#EVALUATE MODEL

# Make predictions on the test set
y_pred = et_model.predict(X_test)

# Print the classification report
print("=== Extra Trees Classifier Report ===")
print(classification_report(y_test, y_pred))

# Display the confusion matrix
ConfusionMatrixDisplay.from_estimator(et_model, X_test, y_test, cmap='Greens')
plt.title("Extra Trees Confusion Matrix")
plt.show()

#Techniques to Improve Efficiency
#a.Dimensionality Reduction

from sklearn.decomposition import PCA

pca = PCA(n_components=5)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

#b.FEATURE ENGINEERING

import pandas as pd
import numpy as np

# Load dataset
url = '/content/water_potability.csv'
df = pd.read_csv(url)

df.head()

print(df.columns)

#FEATURE ENGINEERING

df['TDS_Hardness_Ratio'] = df['Solids'] / (df['Hardness'] + 1)  # Adding 1 to avoid division by zero
df['Sulfate_Turbidity_Ratio'] = df['Sulfate'] / (df['Turbidity'] + 1)

#Add polynomial features for pH, Hardness, and Solids

import pandas as pd
import numpy as np
from sklearn.preprocessing import PolynomialFeatures


columns_to_transform = ['ph', 'Hardness', 'Solids']

# Handle missing values by filling with the mean of each column
df[columns_to_transform] = df[columns_to_transform].fillna(df[columns_to_transform].mean())

# Initialize PolynomialFeatures with degree 2
poly = PolynomialFeatures(degree=2, include_bias=False)

# Fit and transform the selected columns
poly_features = poly.fit_transform(df[columns_to_transform])

# Create a DataFrame with the new polynomial features
poly_columns = poly.get_feature_names_out(columns_to_transform)
poly_df = pd.DataFrame(poly_features, columns=poly_columns)

# Reset the index of both DataFrames to ensure alignment before concatenation
df = df.reset_index(drop=True)
poly_df = poly_df.reset_index(drop=True)

# Concatenate the new polynomial features with the original dataset
df = pd.concat([df, poly_df], axis=1)

# Display the updated DataFrame
print(df.head())

#LOG TRANSFORMATION

print(type(df['Solids']))
print(type(df['Sulfate']))

import pandas as pd
import numpy as np

# Sample data
data = {'ph': [10, 20, 30, 40, 50],
        'Solids_1': [100, 200, 300, 400, 500]}
df = pd.DataFrame(data)

# Apply log transformation to each column separately
df['ph'] = np.log(df['ph'])
df['Solids_1'] = np.log(df['Solids_1'])

print(df)

#Binning pH into categories: Acidic, Neutral, Alkaline

print(df.columns)

print(df.head())

df.columns = df.columns.str.lower()  # Convert all column names to lowercase
print(df.columns)  # Verify again

print(df.columns.tolist())  # Print all column names explicitly as a list

# Create the 'ph_Binned' column by binning the 'ph' values into categories
df['ph_Binned'] = pd.cut(
    df['ph'],
    bins=[0, 6.5, 7.5, 14],                # Define bins: Acidic (<6.5), Neutral (6.5-7.5), Alkaline (>7.5)
    labels=['Acidic', 'Neutral', 'Alkaline']  # Labels for each bin
)

# Display the first few rows to verify the 'ph_Binned' column
print(df[['ph', 'ph_Binned']].head())

# Interaction Features

import pandas as pd

# Load the dataset from a file (update path if necessary)
df = pd.read_csv('water_potability.csv')

# Check for duplicate columns
print("Duplicate columns:", df.columns[df.columns.duplicated()])

# Deduplicate column names by adding suffixes to duplicates
df.columns = pd.Series(df.columns).apply(
    lambda x: f"{x}_{pd.Series(df.columns).value_counts()[x]-1}"
    if pd.Series(df.columns).value_counts()[x] > 1 else x
)

# Verify column names after deduplication
print("Updated columns:", df.columns)

# Handle missing values by filling with the mean
df['Hardness'] = df['Hardness'].fillna(df['Hardness'].mean())
df['Turbidity'] = df['Turbidity'].fillna(df['Turbidity'].mean())

# Create the interaction term
df['Hardness_Turbidity_Interaction'] = df['Hardness'] * df['Turbidity']

# Display the result
print(df[['Hardness', 'Turbidity', 'Hardness_Turbidity_Interaction']].head())

#Encode the pH_Binned feature

import pandas as pd
import numpy as np

# Load the dataset (modify the path if necessary)
df = pd.read_csv('water_potability.csv')

# Check for missing values in the 'ph' column and handle them by filling with the mean
df['ph'] = df['ph'].fillna(df['ph'].mean())

# Create the 'ph_Binned' column by binning the 'ph' values
df['ph_Binned'] = pd.cut(
    df['ph'],
    bins=[0, 6.5, 7.5, 14],                # Define the bins
    labels=['Acidic', 'Neutral', 'Alkaline']  # Labels for each bin
)

# Display the first few rows to verify 'ph_Binned'
print(df[['ph', 'ph_Binned']].head())

# Add 'Unknown' to the categories
df['ph_Binned'] = df['ph_Binned'].cat.add_categories('Unknown')

# Now, fill missing values with 'Unknown'
df['ph_Binned'] = df['ph_Binned'].fillna('Unknown')

# Convert to object type temporarily
df['ph_Binned'] = df['ph_Binned'].astype('object')

# Now, fill missing values with 'Unknown'
df['ph_Binned'] = df['ph_Binned'].fillna('Unknown')

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Handle missing values in 'ph' by filling with the mean
df['ph'] = df['ph'].fillna(df['ph'].mean())

# Create the 'ph_Binned' column by binning the 'ph' values
df['ph_Binned'] = pd.cut(
    df['ph'],
    bins=[0, 6.5, 7.5, 14],
    labels=['Acidic', 'Neutral', 'Alkaline']
)

# Add 'Unknown' as a new category to 'ph_Binned'
df['ph_Binned'] = df['ph_Binned'].cat.add_categories('Unknown')

# Handle missing values in 'ph_Binned' by filling with 'Unknown'
df['ph_Binned'] = df['ph_Binned'].fillna('Unknown')

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Encode the 'ph_Binned' column
df['ph_Binned_Encoded'] = label_encoder.fit_transform(df['ph_Binned'])

# Display the first few rows to verify
print(df[['ph', 'ph_Binned', 'ph_Binned_Encoded']].head())

#FEATURE SCALING

from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
scaled_features = scaler.fit_transform(df[['Hardness', 'Solids', 'Sulfate', 'Conductivity']])

# Convert back to DataFrame
scaled_df = pd.DataFrame(scaled_features, columns=['Hardness_Scaled', 'Solids_Scaled', 'Sulfate_Scaled', 'Conductivity_Scaled'])
df = pd.concat([df, scaled_df], axis=1)

#FINAL FEATURE SELECTION

print(df.columns)

import pandas as pd
import numpy as np
from sklearn.preprocessing import LabelEncoder

# Load the dataset
df = pd.read_csv('water_potability.csv')

# Handle missing values in 'ph' by filling with the mean
df['ph'] = df['ph'].fillna(df['ph'].mean())

# Create the 'ph_Binned' column by binning the 'ph' values
df['ph_Binned'] = pd.cut(
    df['ph'],
    bins=[0, 6.5, 7.5, 14],
    labels=['Acidic', 'Neutral', 'Alkaline']
)

# Add 'Unknown' as a new category to 'ph_Binned'
df['ph_Binned'] = df['ph_Binned'].cat.add_categories('Unknown')

# Handle missing values in 'ph_Binned' by filling with 'Unknown'
df['ph_Binned'] = df['ph_Binned'].fillna('Unknown')

# Initialize the LabelEncoder
label_encoder = LabelEncoder()

# Encode the 'ph_Binned' column
df['ph_Binned_Encoded'] = label_encoder.fit_transform(df['ph_Binned'])

# Create additional required columns (example)
# You need to define how these columns are created. Here are some examples:
df['Solids_Hardness_Ratio'] = df['Solids'] / df['Hardness']
df['Sulfate_Turbidity_Ratio'] = df['Sulfate'] / df['Turbidity']
df['Log_Solids'] = np.log1p(df['Solids'])
df['Log_Sulfate'] = np.log1p(df['Sulfate'])
df['Hardness_Turbidity_Interaction'] = df['Hardness'] * df['Turbidity']

# List of final features you want to select
final_features = [
    'Solids_Hardness_Ratio', 'Sulfate_Turbidity_Ratio', 'Log_Solids', 'Log_Sulfate',
    'Hardness_Turbidity_Interaction', 'ph_Binned_Encoded'
]

# Check if all columns in final_features exist in the DataFrame
missing_columns = [col for col in final_features if col not in df.columns]
if missing_columns:
    print(f"Missing columns: {missing_columns}")
else:
    # Select the features and target variable
    X = df[final_features]
    y = df['Potability']

    # Display the selected features
    print(X.head())

# //MODEL DEPLOYMENT//

#Streamlit library helps you quickly create interactive web apps for ML models.

!pip install streamlit

#Train and Save Your Model

# Step 1: Import necessary libraries
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score
import pickle
from google.colab import files

# Step 2: Upload your dataset
print("Please upload your dataset (e.g., water_potability.csv):")
uploaded = files.upload()

# Step 3: Load the dataset
filename = list(uploaded.keys())[0]
df = pd.read_csv(filename)

# Step 4: Handle missing values
df.fillna(df.median(), inplace=True)

# Step 5: Split dataset into features and target variable
X = df.drop('Potability', axis=1)
y = df['Potability']

# Step 6: Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 7: Train a Random Forest Classifier
clf = RandomForestClassifier(random_state=42)
clf.fit(X_train, y_train)

# Step 8: Evaluate the model
y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Model Accuracy: {accuracy:.2f}")

# Step 9: Save the model to a file
with open('model.pkl', 'wb') as model_file:
    pickle.dump(clf, model_file)

# Step 10: Download the saved model
print("Downloading the trained model (model.pkl)...")
files.download('model.pkl')

# Step 11: Write the Streamlit app script
app_code = """
import streamlit as st
import pandas as pd
import pickle

# Load the model
with open('model.pkl', 'rb') as model_file:
    model = pickle.load(model_file)

st.title("Water Potability Prediction")

# File upload
uploaded_file = st.file_uploader("Upload a CSV file", type=["csv"])
if uploaded_file:
    input_df = pd.read_csv(uploaded_file)
    st.write("Uploaded Dataset:", input_df)

    # Make predictions
    predictions = model.predict(input_df)
    input_df['Potability_Prediction'] = predictions
    st.write("Prediction Results:", input_df)

    # Download predictions
    st.download_button(
        label="Download Predictions",
        data=input_df.to_csv(index=False).encode('utf-8'),
        file_name='predictions.csv',
        mime='text/csv'
    )
"""

# Step 12: Save the app script to a file
with open('app.py', 'w') as app_file:
    app_file.write(app_code)

# Step 13: Download the Streamlit app script
print("Downloading the Streamlit app script (app.py)...")
files.download('app.py')

# Step 14: Create a requirements.txt file
requirements = """streamlit
pandas
scikit-learn
numpy
"""
with open('requirements.txt', 'w') as req_file:
    req_file.write(requirements)

# Step 15: Download the requirements file
print("Downloading the requirements file (requirements.txt)...")
files.download('requirements.txt')

print("All files are ready for deployment!")

!pip install -r requirements.txt

!streamlit run app.py

!python -m venv venv
!source venv/bin/activate  # On Windows, use venv\Scripts\activate
!pip install -r requirements.txt

